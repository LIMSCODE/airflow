카프카(실시간데이터저장,처리)- 호환성, 대용량데이터처리
nifi (데이터흐름 시각적관리 -카프카의 컨슈머, 데이터프로듀서역할, 전송, 자동화) , 
airflow(Kafka로 데이터전송 스케쥴링 /배치) -nifi가 데이터흐름관리, aiflow는 배치 (상호보완)


X 카프카를 ec2에설치해서 자바로받을지, 윈도우에 설치할지 결정
리눅스환경에서 파이썬으로 hbase 혹은 hdfs에 저장시킬지 결정
airflow는 파이썬기반이여서 파이썬으로해야할듯.


=================================이방법으로 프로젝트진행함
1.윈도우에서 WLS 설정
@@@이거먼저 2.WLS에서 airflow설치 https://blog.naver.com/occidere/221773113221

3. 챗지피티에 kafka airflow 프로젝트 검색
-> 만든 DAG가 produce태스크에서 kafka토픽에 메시지생성, consume태스크에서 메시지 소비
-> (카프카설치,시작,토픽생성,메시지전송,확인/aiflow설치,시작,DAG생성)
-> (DAG코드작성,코드에 taskId지정, aiflow웹이서 DAG확인,실행가능)
4. airflow DB도 만들어야하는가 https://weejw.tistory.com/537
4. WSL에 airflow설치 https://blog.naver.com/occidere/221773113221

//cmd에서 관리자로 실행해야됨
C:\Users\user>wsl --install -d Ubuntu 하면
root@DESKTOP-L3AT3SL:~# ls로 접속됨 #########################################

# wget apache.mirror.cdnetworks.com/kafka/3.8.0/kafka_2.13-3.8.0.tgz 
tar -xzf kafka_2.13-3.8.0.tgz

//자바설치해야함 
 환경 변수 확인
필요한 환경 변수들이 설정되어 있는지 확인합니다. 특히, Java 환경 변수가 올바르게 설정되어 있어야 합니다. Java가 설치되어 있는지 확인하고 JAVA_HOME을 설정합니다.

bash
코드 복사
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin
Java가 설치되어 있지 않은 경우, 설치가 필요합니다:

bash
코드 복사
sudo apt-get update
sudo apt-get install default-jdk

//주키퍼 기본포트 열어줘야함 -방화벽 인바운드, 아웃바운드설정해줌 2181
netstat -ano | find "2030"


//kafka 실행
zookeeper 서버가 실행되어 있는 상태에서 cmd를 하나 더 열어 kafka 서버를 실행합니다.





//WSL, 도커, 파이썬, airflow, 카프카
https://dev-records.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-Kafka-%EA%B0%84%EB%8B%A8%ED%95%9C-%EC%98%88%EC%A0%9C
================================== 기타자료조사
//카프카 to HBase (니피)
https://wave35.tistory.com/51?category=1097018
//HBase설치
https://wave35.tistory.com/6

//니피서치, 카프카, hdfs연결 (위지원 / WSL2에서설치)
https://weejw.tistory.com/599


//스프링부트, ec2 카프카설치
https://devbksheen.tistory.com/entry/Kafka-Spring-Boot%EC%97%90-Kafka%EB%A5%BC-%EC%97%B0%EB%8F%99?category=1024738
https://devbksheen.tistory.com/125
https://devbksheen.tistory.com/124
//카프카 nifi 프로젝트(롤 /스프링부트 / ec2 카프카설치)
https://gyuturn.tistory.com/33



===========================윈ㄷ우
//nifi 윈도우설치
//kafka 윈도우설치
https://herojoon-dev.tistory.com/118

//카프카 클로스터 구성
https://hbcha0916.tistory.com/65
//니피 클러스터 구성
https://hbcha0916.tistory.com/64

//실무예제로배우는 데이터공학 -카프카, nifi, airflow
https://www.yes24.com/Product/Goods/102628179

카프카스트림즈(카프카와만 관련됨) -카프카에저장된 데이터를 실시간처리 (
카프카사용이유 -호환성, 대용량데이터처리
https://cloud101.tistory.com/5


github.com/onlybooks/kafka

[[[[[ 2장 카프카설치 ]]]]]]]
프로듀서는 카프카와 통신하며 메시지를 보냄 /컨슈머는 카프카와 통신하며 메신져 가져옴 #######################
주키퍼는 카프카의메타데이터를 주키퍼에 저장하고 카프카의 상태관리 ##################################
//주키퍼설치 67p
//카프카설치/환경설정/실행 ###########################################

==========================================
[[[[[ 7장 카프카를 활용한 데이터 파이프라인 구축 ]]]]]]
아파치 나이파이는 데이터흐름 자동실행하는 어플리케이션 /웹기반 인터페이스
파일비트를이용한 메시지전송
파일비트실행시 카프카의peter-log토픽으로 로그전송 / 메시지유입확인위해 콘솔컨슈머로 확인

나이파이로 메시지가져오기 /컨슈머역할로 이용
나이파이설치 273p /웹인터페이스접근 - http://peter-kafka001:8080/nifi
나이파이의 putHDFS 프로세서등을 추가해 하둡 등의 저장소에 저장 ################################

엘라스틱서치 설정 -283p /-전문검색질의를 이용한 실시간분석 /나이파이 이용해 데이터전송
키바나설치 288p /엘라스틱서치에 저장된  데이터확인,분석 어플
 
카프카콘솔컨슈머에서 토픽 hostname:peter-kafka001인지 확인가능 297p
실시간 로그데이터처리,분석작업에 도움됨









==========================================
[[[[[ 8장 카프카 스트림즈 API ]]]]]
스트림프로세싱 : 데이터지속적 유입,나가는과정에서 분석시스템도달하자마자 분석,질의수행  ######################
스트림(저장후분석하지않으므로 더많은데이터분석, 인프라독립-무상태)+배치(상태기반) 
실시간,정확성 동시에챙김
반면 배치는 이미저장된데이터기반으로 특정시간에철리
//카프카스트림즈 카프카에저장된 데이터처리,분석 환경설정 -310p


//(프로젝트) 행분리 예제프로그램 만들기 321p
한쪽토픽에서 읽은데이터를 공백기준으로분리해서 다른토픽에저장
pipe-complete.java를 보가해서 LineSplit.java 만듬
행분리코드작성 / KStream메소드
java -Dexec.mainClass=myapps.LineSplit으로 실행함 

LineSplit-complete.java 에 완성된코드있음 324p
console-producer, console-consumer 시작함
console-producer 에 문장3개입력
console-consumer에 읽을토픽ㅇ르 streams-linesplit-output으로지정후 출력하면 빈칸기준으로 문장분리됨


//(프로젝트) 단어 빈도수세기 예제프로그램
LineSplit-complete.java 복사해서 WordCount.java로 만듬 327p
toStream으로 테이블데이터를 스트림으로변환
to("streams-wordcount-output")으로 변환한 스트림을 토픽에저장한다

WordCount-complete.java로 streams-plaintext-input토픽에서 데이터읽어들임
키에따라 count-store-repartion에 배열형태로저장
키없는 데이터거럴냄

count-store-repartion 토픽에서 데이터를 읽어서 KSTREAM-AGGREGATE=0000000003로 전달한다
000003노드는 새로운스트림들어올때마다 counts-store에서 해당키 현재값을 확인후 같은키의 스트림이면 값을 증가시켜
streams-wordcount-output에 저장한다...

-Dexec.mainClass=myapps.WordCount로 출력한다
이상태에서 console-consumer, consoe-producer(읽을토픽은streams-wordcount-output으로적용,키도입력)실행 
console-producer에서 문장입력시 console-consumer에서 워드와 빈도수 변하는것 확인할수있다... ######################

카프카스트림즈API이용시 스파크,스톰같은 별도의 스트리밍엔진 사용하지않고 간단히 실시간분석수행가능 #########################

==========================================
[[[[[ 9장 카프카SQL을이용한 스트리밍처리 ]]]]]

카프카SQL(KSQL)로 저장기간에 관계없이 스트리밍, 배치처리 동시에 할수있는 데이터분석방법
카프카사용시 스트리밍플랫폼 이외로 배치시스템으로 장기데이터처리 (별도 계산시스템 부담) 

계산프로그램 장기,단기 저장기간 구분없이 사용하는것 - 카파 아키텍쳐라 하고, 이때 KSQL사용
카파아키텍쳐에 카프카적용해서 카프카의 토픽을 저장하고 KSQL로 가져옴
단기데이터는 카프카토픽에저장하고 스파크,스톰을 통해 데이터만들어내고
장기데이터는 하둡에복사한다음 맵리듀스,스파크로 결과만들어내야함
필요데이터가져오는부분을 조회대신 계산으로 전환함으로써 데이터관리단순화

람다아키텍쳐와 차이점은 장기데이터 배치로 따로저장해서 가져오는게아닌 
장기데이터 조회필요시에 계산해서 결과를 그떄그때전달하는것
  
쿼리결과로 스트림생성 346p
CREATE STREAM stream_name WITH KAFKA_TOPIC = 'my-users-topic') (테이블에 사용할 토픽)
AS SELECT (스트림 쿼리결과로 바로테이블생성)

=====================
//도커 사용한 KSQL설치 349p
ksql/docs/quickstart/ docker-compose.yml으로 클러스터생성한다 351p
docker-compose up -d 로 KSQL클러스터를 실행함 
docker-compose 파일에서 실행한순서대로 주키퍼서버>카프카서버>스키마 레지스트리서버 순으로 이미지를내려받고
컨테이너를 실행한다.

//KSQL을 이용한 스트림분석
주로여성이접근하는 페이지정보는? - Users, PageView 토픽기반으로 pageViews라는스트림과 Users테이블생성
KSQL docker-compose.yml 파일에서 데이터생성부분
ksql-datagen-pageviews /topic=pageviews
ksql-datagen-users /topc=users
데이터생성확인 docker-compose exec kafka kafka-topoics --list --zookeeper zookeeper:32181 토픽리스트확인
pageviews, users토픽생성됨

//이제 기본스트림,테이블생성 360p
docker-compose exec ksql-cli ksql-cli local --bootstrap-server kafka:29092 로 ksql클라이언트띄움
CREATE STREAM pageviews_original ~로 스트림생성
DESCRIBE PAGEVIEWS_ORIGINAL;
CREATE TABLE users_original로 사용자테이블만듬
describe USERS_ORIGINAL;

//쿼리로 새로운스트림,테이블 생성 362p
주로여성이접속하는 페이지정보 답을구하기위해 pageView스트림, Users테이블에서 쿼리문만들고 스트림화
CREATE STEAM pageviews_female AS 
SELECT users_original.userid AS userid, pageid, regionid  FROM pageviews_original ####
LEFT JOIN users_original #####

카프카토픽에서 데이터꺼내려면 별도앱개발해야했는데 
간단한 쿼리문만으로도 기본분석뿐아니라 복잡분석도 간능


































[[[[[ 클라우드에서 펍/섭 기반의 실시간 데이터분석 ]]]]]
